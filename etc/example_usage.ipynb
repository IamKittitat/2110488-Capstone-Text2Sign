{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIP AND IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch==2.2.2\n",
    "# %pip install torchtext==0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a video given a tensor of joints, a file path, video name and references/sequence ID\n",
    "def plot_skeletons_video(joints, file_path, video_name, references=None, skip_frames=1, sequence_ID=None, pad_token: int = 0):\n",
    "    # Create video template\n",
    "    FPS = 25 // skip_frames\n",
    "    video_file = file_path + \"/{}.mp4\".format(video_name.split(\".\")[0])\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "\n",
    "    frame_size = (650, 650) if references is None else (1300, 650)\n",
    "    video = cv2.VideoWriter(video_file, fourcc, float(FPS), frame_size, True)\n",
    "\n",
    "    for frame_index, frame_joints in enumerate(joints):\n",
    "        # Reached padding\n",
    "        if pad_token in frame_joints:\n",
    "            continue\n",
    "\n",
    "        # Initialise frame of white\n",
    "        frame = np.ones((650, 650, 3), np.uint8) * 255\n",
    "\n",
    "        # Cut off the percent_tok, multiply by 3 to restore joint size\n",
    "        frame_joints = frame_joints[:] * 3\n",
    "        frame_joints_2d = np.reshape(frame_joints, (50, 3))[:, :2]\n",
    "\n",
    "        # Draw the frame given 2D joints and add text\n",
    "        draw_frame_2D(frame, frame_joints_2d)\n",
    "        cv2.putText(frame, \"Predicted Sign Pose\", (180, 600), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # If reference is provided, create and concatenate on the end\n",
    "        if references is not None:\n",
    "            # Extract the reference joints\n",
    "            ref_joints = references[frame_index]\n",
    "            # Initialise frame of white\n",
    "            ref_frame = np.ones((650, 650, 3), np.uint8) * 255\n",
    "\n",
    "            # Cut off the percent_tok and multiply each joint by 3 (as was reduced in training files)\n",
    "            ref_joints = ref_joints[:] * 3\n",
    "            ref_joints_2d = np.reshape(ref_joints, (50, 3))[:, :2]\n",
    "\n",
    "            # Draw these joints on the frame and add text\n",
    "            draw_frame_2D(ref_frame, ref_joints_2d, offset=(0, -20))\n",
    "            cv2.putText(ref_frame, \"Ground Truth Pose\", (190, 600), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "            # Concatenate the two frames\n",
    "            frame = np.concatenate((frame, ref_frame), axis=1)\n",
    "\n",
    "            # Add the sequence ID to the frame\n",
    "            sequence_ID_write = \"Sequence ID: \" + sequence_ID.split(\"/\")[-1]\n",
    "            cv2.putText(frame, sequence_ID_write, (700, 635), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "\n",
    "        # Write the video frame\n",
    "        video.write(frame)\n",
    "    # Release the video\n",
    "    video.release()\n",
    "\n",
    "# This is the format of the 3D data, outputted from the Inverse Kinematics model\n",
    "def getSkeletalModelStructure():\n",
    "    # Definition of skeleton model structure:\n",
    "    #   The structure is an n-tuple of:\n",
    "    #\n",
    "    #   (index of a start point, index of an end point, index of a bone)\n",
    "    #\n",
    "    #   E.g., this simple skeletal model\n",
    "    #\n",
    "    #             (0)\n",
    "    #              |\n",
    "    #              |\n",
    "    #              0\n",
    "    #              |\n",
    "    #              |\n",
    "    #     (2)--1--(1)--1--(3)\n",
    "    #      |               |\n",
    "    #      |               |\n",
    "    #      2               2\n",
    "    #      |               |\n",
    "    #      |               |\n",
    "    #     (4)             (5)\n",
    "    #\n",
    "    #   has this structure:\n",
    "    #\n",
    "    #   (\n",
    "    #     (0, 1, 0),\n",
    "    #     (1, 2, 1),\n",
    "    #     (1, 3, 1),\n",
    "    #     (2, 4, 2),\n",
    "    #     (3, 5, 2),\n",
    "    #   )\n",
    "    #\n",
    "    #  Warning 1: The structure has to be a tree.\n",
    "    #  Warning 2: The order isn't random. The order is from a root to lists.\n",
    "    #\n",
    "\n",
    "    return (\n",
    "        # head\n",
    "        (0, 1, 0),\n",
    "        # left shoulder\n",
    "        (1, 2, 1),\n",
    "        # left arm\n",
    "        (2, 3, 2),\n",
    "        # (3, 4, 3),\n",
    "        # Changed to avoid wrist, go straight to hands\n",
    "        (3, 29, 3),\n",
    "        # right shoulder\n",
    "        (1, 5, 1),\n",
    "        # right arm\n",
    "        (5, 6, 2),\n",
    "        # (6, 7, 3),\n",
    "        # Changed to avoid wrist, go straight to hands\n",
    "        (6, 8, 3),\n",
    "        # left hand - wrist\n",
    "        # (7, 8, 4),\n",
    "        # left hand - palm\n",
    "        (8, 9, 5),\n",
    "        (8, 13, 9),\n",
    "        (8, 17, 13),\n",
    "        (8, 21, 17),\n",
    "        (8, 25, 21),\n",
    "        # left hand - 1st finger\n",
    "        (9, 10, 6),\n",
    "        (10, 11, 7),\n",
    "        (11, 12, 8),\n",
    "        # left hand - 2nd finger\n",
    "        (13, 14, 10),\n",
    "        (14, 15, 11),\n",
    "        (15, 16, 12),\n",
    "        # left hand - 3rd finger\n",
    "        (17, 18, 14),\n",
    "        (18, 19, 15),\n",
    "        (19, 20, 16),\n",
    "        # left hand - 4th finger\n",
    "        (21, 22, 18),\n",
    "        (22, 23, 19),\n",
    "        (23, 24, 20),\n",
    "        # left hand - 5th finger\n",
    "        (25, 26, 22),\n",
    "        (26, 27, 23),\n",
    "        (27, 28, 24),\n",
    "        # right hand - wrist\n",
    "        # (4, 29, 4),\n",
    "        # right hand - palm\n",
    "        (29, 30, 5),\n",
    "        (29, 34, 9),\n",
    "        (29, 38, 13),\n",
    "        (29, 42, 17),\n",
    "        (29, 46, 21),\n",
    "        # right hand - 1st finger\n",
    "        (30, 31, 6),\n",
    "        (31, 32, 7),\n",
    "        (32, 33, 8),\n",
    "        # right hand - 2nd finger\n",
    "        (34, 35, 10),\n",
    "        (35, 36, 11),\n",
    "        (36, 37, 12),\n",
    "        # right hand - 3rd finger\n",
    "        (38, 39, 14),\n",
    "        (39, 40, 15),\n",
    "        (40, 41, 16),\n",
    "        # right hand - 4th finger\n",
    "        (42, 43, 18),\n",
    "        (43, 44, 19),\n",
    "        (44, 45, 20),\n",
    "        # right hand - 5th finger\n",
    "        (46, 47, 22),\n",
    "        (47, 48, 23),\n",
    "        (48, 49, 24),\n",
    "    )\n",
    "\n",
    "# Draw a line between two points, if they are positive points\n",
    "def draw_line(im, joint1, joint2, c=(0, 0, 255), t=1, width=3):\n",
    "    thresh = -100\n",
    "    if joint1[0] > thresh and joint1[1] > thresh and joint2[0] > thresh and joint2[1] > thresh:\n",
    "\n",
    "        center = (int((joint1[0] + joint2[0]) / 2), int((joint1[1] + joint2[1]) / 2))\n",
    "\n",
    "        length = int(math.sqrt(((joint1[0] - joint2[0]) ** 2) + ((joint1[1] - joint2[1]) ** 2)) / 2)\n",
    "\n",
    "        angle = math.degrees(math.atan2((joint1[0] - joint2[0]), (joint1[1] - joint2[1])))\n",
    "\n",
    "        cv2.ellipse(im, center, (width, length), -angle, 0.0, 360.0, c, -1)\n",
    "\n",
    "# Draw the frame given 2D joints that are in the Inverse Kinematics format\n",
    "def draw_frame_2D(frame, joints, offset: tuple[int, int] = (0, 0)):\n",
    "    # Line to be between the stacked\n",
    "    draw_line(frame, [1, 650], [1, 1], c=(0, 0, 0), t=1, width=1)\n",
    "    # Give an offset to center the skeleton around\n",
    "\n",
    "    # Get the skeleton structure details of each bone, and size\n",
    "    skeleton = getSkeletalModelStructure()\n",
    "    skeleton = np.array(skeleton)\n",
    "\n",
    "    number = skeleton.shape[0]\n",
    "\n",
    "    # Increase the size and position of the joints\n",
    "    joints = joints * 10 * 12 * 2\n",
    "    joints = joints + np.ones((50, 2)) * (0,0)\n",
    "\n",
    "    # Loop through each of the bone structures, and plot the bone\n",
    "    for j in range(number):\n",
    "\n",
    "        c = get_bone_colour(skeleton, j)\n",
    "\n",
    "        draw_line(\n",
    "            frame,\n",
    "            [joints[skeleton[j, 0]][0], joints[skeleton[j, 0]][1]],\n",
    "            [joints[skeleton[j, 1]][0], joints[skeleton[j, 1]][1]],\n",
    "            c=c,\n",
    "            t=1,\n",
    "            width=1,\n",
    "        )\n",
    "\n",
    "# get bone colour given index\n",
    "def get_bone_colour(skeleton, j):\n",
    "    bone = skeleton[j, 2]\n",
    "\n",
    "    if bone == 0:  # head\n",
    "        c = (0, 153, 0)\n",
    "    elif bone == 1:  # Shoulder\n",
    "        c = (0, 0, 255)\n",
    "\n",
    "    elif bone == 2 and skeleton[j, 1] == 3:  # left arm\n",
    "        c = (0, 102, 204)\n",
    "    elif bone == 3 and skeleton[j, 0] == 3:  # left lower arm\n",
    "        c = (0, 204, 204)\n",
    "\n",
    "    elif bone == 2 and skeleton[j, 1] == 6:  # right arm\n",
    "        c = (0, 153, 0)\n",
    "    elif bone == 3 and skeleton[j, 0] == 6:  # right lower arm\n",
    "        c = (0, 204, 0)\n",
    "\n",
    "    # Hands\n",
    "    elif bone in [5, 6, 7, 8]:\n",
    "        c = (0, 0, 255)\n",
    "    elif bone in [9, 10, 11, 12]:\n",
    "        c = (51, 255, 51)\n",
    "    elif bone in [13, 14, 15, 16]:\n",
    "        c = (255, 0, 0)\n",
    "    elif bone in [17, 18, 19, 20]:\n",
    "        c = (204, 153, 255)\n",
    "    elif bone in [21, 22, 23, 24]:\n",
    "        c = (51, 255, 255)\n",
    "\n",
    "    return c\n",
    "\n",
    "# Apply DTW to the produced sequence, so it can be visually compared to the reference sequence\n",
    "def alter_DTW_timing(pred_seq, ref_seq):\n",
    "\n",
    "    # Define a cost function\n",
    "    euclidean_norm = lambda x, y: np.sum(np.abs(x - y))\n",
    "\n",
    "    # Cut the reference down to the max count value\n",
    "    _, ref_max_idx = torch.max(ref_seq[:, -1], 0)\n",
    "    if ref_max_idx == 0:\n",
    "        ref_max_idx += 1\n",
    "    # Cut down frames by counter\n",
    "    ref_seq = ref_seq[:ref_max_idx, :].cpu().numpy()\n",
    "\n",
    "    # Cut the hypothesis down to the max count value\n",
    "    _, hyp_max_idx = torch.max(pred_seq[:, -1], 0)\n",
    "    if hyp_max_idx == 0:\n",
    "        hyp_max_idx += 1\n",
    "    # Cut down frames by counter\n",
    "    pred_seq = pred_seq[:hyp_max_idx, :].cpu().numpy()\n",
    "\n",
    "    # Run DTW on the reference and predicted sequence\n",
    "    d, cost_matrix, acc_cost_matrix, path = dtw(ref_seq[:, :-1], pred_seq[:, :-1], dist=euclidean_norm)\n",
    "\n",
    "    # Normalise the dtw cost by sequence length\n",
    "    d = d / acc_cost_matrix.shape[0]\n",
    "\n",
    "    # Initialise new sequence\n",
    "    new_pred_seq = np.zeros_like(ref_seq)\n",
    "    # j tracks the position in the reference sequence\n",
    "    j = 0\n",
    "    skips = 0\n",
    "    squeeze_frames = []\n",
    "    for i, pred_num in enumerate(path[0]):\n",
    "\n",
    "        if i == len(path[0]) - 1:\n",
    "            break\n",
    "\n",
    "        if path[1][i] == path[1][i + 1]:\n",
    "            skips += 1\n",
    "\n",
    "        # If a double coming up\n",
    "        if path[0][i] == path[0][i + 1]:\n",
    "            squeeze_frames.append(pred_seq[i - skips])\n",
    "            j += 1\n",
    "        # Just finished a double\n",
    "        elif path[0][i] == path[0][i - 1]:\n",
    "            new_pred_seq[pred_num] = avg_frames(squeeze_frames)\n",
    "            squeeze_frames = []\n",
    "        else:\n",
    "            new_pred_seq[pred_num] = pred_seq[i - skips]\n",
    "\n",
    "    return new_pred_seq, ref_seq, d\n",
    "\n",
    "# Find the average of the given frames\n",
    "def avg_frames(frames):\n",
    "    frames_sum = np.zeros_like(frames[0])\n",
    "    for frame in frames:\n",
    "        frames_sum += frame\n",
    "\n",
    "    avg_frame = frames_sum / len(frames)\n",
    "    return avg_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, skel_file='../T2S-GPT/data/alldata/train.skels', text_file='../T2S-GPT/data/alldata/train.txt', vocab=None, seq_length=200, text_length=30, sign_language_dim=150, window_size=200):\n",
    "        self.seq_length = seq_length\n",
    "        self.text_length = text_length\n",
    "        self.sign_language_dim = sign_language_dim\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.min_value = float(\"inf\")\n",
    "        self.max_value = -float(\"inf\")\n",
    "\n",
    "        # Read and process skeleton data\n",
    "        with open(skel_file, 'r') as f:\n",
    "            self.sign_language_data = f.readlines()\n",
    "            self.sign_language_data = [line.strip().split(\" \") for line in self.sign_language_data]\n",
    "            self.sign_language_data = [[float(val) for val in line] for line in self.sign_language_data]\n",
    "            self.sign_language_data = [torch.tensor(line).reshape(-1, self.sign_language_dim + 1)[:, :-1] for line in self.sign_language_data]\n",
    "\n",
    "        # Read and process text data\n",
    "        with open(text_file, 'r') as f:\n",
    "            self.text_data = [line.strip().split() for line in f]\n",
    "\n",
    "        # Filter sequences shorter than window size\n",
    "        self.sign_language_data, self.text_data = zip(*[\n",
    "            (line, self.text_data[idx])\n",
    "            for idx, line in enumerate(self.sign_language_data)\n",
    "            if line.shape[0] > self.window_size\n",
    "        ])\n",
    "\n",
    "        # Normalize the skeleton data\n",
    "        for line in self.sign_language_data:\n",
    "            self.min_value = min(self.min_value, line.min())\n",
    "            self.max_value = max(self.max_value, line.max())\n",
    "        self.sign_language_data = [(line - self.min_value) / (self.max_value - self.min_value) for line in self.sign_language_data]\n",
    "\n",
    "        # Build or use existing vocabulary for text data\n",
    "        self.vocab = vocab if vocab else self.build_vocab(self.text_data)\n",
    "\n",
    "    def build_vocab(self, text_data):\n",
    "        def yield_tokens(data):\n",
    "            for text in data:\n",
    "                yield text\n",
    "        return build_vocab_from_iterator(yield_tokens(text_data))\n",
    "\n",
    "    def preprocess_data(self, X_T):\n",
    "        padded_X_T = F.pad(X_T, (0, 512 - X_T.size(-1)), \"constant\", 0)\n",
    "        return padded_X_T\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sign_language_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_length = self.sign_language_data[idx].shape[0]\n",
    "        if seq_length < self.window_size:\n",
    "            raise ValueError(f\"Sequence at index {idx} is shorter than window size {self.window_size}\")\n",
    "\n",
    "        start_index = torch.randint(0, seq_length - self.window_size, (1,)).item()\n",
    "        end_index = start_index + self.window_size\n",
    "        sign_language_sequence = self.sign_language_data[idx][start_index:end_index]\n",
    "\n",
    "        spoken_language_text = self.text_data[idx]\n",
    "        spoken_language_tensor = torch.tensor([self.vocab[word] for word in spoken_language_text[:self.text_length]], dtype=torch.long)\n",
    "\n",
    "        # Generate placeholders for additional variables\n",
    "        skel_indices = torch.randint(0, 512, (self.window_size,))  # Example placeholder\n",
    "        skel_reconstructed = sign_language_sequence.clone().unsqueeze(0)  # Adding batch dimension for 3D\n",
    "        indices_prediction = skel_indices.clone().unsqueeze(0)            # Adding batch dimension for 2D\n",
    "        sign_prediction = spoken_language_tensor.clone().unsqueeze(0).unsqueeze(-1)  # Making it 3D: (1, text_length, 1)\n",
    "        converted_skel = sign_language_sequence.clone()\n",
    "        converted_skel_reconstructed = skel_reconstructed.clone()\n",
    "\n",
    "        return {\n",
    "            'sign_language_sequence': sign_language_sequence.to(device),\n",
    "            'spoken_language_text': spoken_language_tensor.to(device),\n",
    "            'skel_indices': skel_indices.to(device),\n",
    "            'skel_reconstructed': skel_reconstructed.to(device),\n",
    "            'indices_prediction': indices_prediction.to(device),\n",
    "            'sign_prediction': sign_prediction.to(device),\n",
    "            'converted_skel': converted_skel.to(device),\n",
    "            'converted_skel_reconstructed': converted_skel_reconstructed.to(device)\n",
    "        }\n",
    "\n",
    "# Instantiate your dataset to check it works\n",
    "dataset = SignLanguageDataset()\n",
    "\n",
    "# Fetch an item to verify\n",
    "data_index = 8\n",
    "item = dataset[data_index]\n",
    "\n",
    "text = item[\"spoken_language_text\"]\n",
    "print(f'Text: \"{item[\"spoken_language_text\"]}\"')\n",
    "print(\"-\" * 50)\n",
    "skel = item[\"sign_language_sequence\"]\n",
    "print(f'Skel: \"{item[\"sign_language_sequence\"]}\"')\n",
    "print(f'Skel: \"{item[\"sign_language_sequence\"].shape}\"')\n",
    "print(\"-\" * 50)\n",
    "skel_indices = item['skel_indices']\n",
    "print(f\"Skeleton indices: {item['skel_indices']}\")\n",
    "print(f\"Skeleton indices: {item['skel_indices'].shape}\")\n",
    "print(\"-\" * 50)\n",
    "skel_reconstructed = item['skel_reconstructed']\n",
    "print(f\"Reconstructed skeleton: {item['skel_reconstructed']}\")\n",
    "print(f\"Reconstructed skeleton: {item['skel_reconstructed'].shape}\")\n",
    "print(\"-\" * 50)\n",
    "indices_prediction = item['indices_prediction']\n",
    "print(f\"Indices prediction: {item['indices_prediction']}\")\n",
    "print(f\"Indices prediction: {item['indices_prediction'].shape}\")\n",
    "print(\"-\" * 50)\n",
    "sign_prediction = item['sign_prediction']\n",
    "print(f\"Sign prediction: {item['sign_prediction']}\")\n",
    "print(f\"Sign prediction: {item['sign_prediction'].shape}\")\n",
    "print(\"-\" * 50)\n",
    "converted_skel = item['converted_skel']\n",
    "print(f\"Converted skeleton: {item['converted_skel']}\")\n",
    "print(\"-\" * 50)\n",
    "converted_skel_reconstructed = item['converted_skel_reconstructed']\n",
    "print(f\"Converted skeleton reconstructed: {item['converted_skel_reconstructed']}\")\n",
    "\n",
    "# from capstone_utils.plot_skeletons import plot_skeletons_video\n",
    "plot_skeletons_video(converted_skel, '.', 'rose.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
